{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iADUEmvtj5gFearPIJ_eVjdk_2phlpoO","timestamp":1704792242842}],"gpuClass":"premium"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","metadata":{"id":"aP1zAHX4S70e"},"source":["# **Training RoBERTa using Hugging Face Transformers**\n","\n","Lecturer: Hieu Tran\n","<br>\n","\n","This notebook is used to pre-train Transformer-based models using [Huggingface](https://huggingface.co/transformers/) on your own dataset.\n","\n","With the AutoClasses functionality, we can reuse the code on a large number of Transformer-based models!\n","\n","This notebook is designed to:\n","\n","* **Train a Transformer-based model from scratch on a text corpus.** This notebook also covers training a tokenizer for your own language/domain. The pre-training objective used in this notebook is Masked Language Modeling (MLM).\n","\n","* **Use an already pre-trained Transformer-based model and fine-tune it on your own dataset.** The fine-tuning loss used in this notebook is Cross-entropy.\n","\n","\n","<br>\n","\n","## **What should I know for this notebook?**\n","\n","Since I am using PyTorch to fine-tune Transformer-based models, any knowledge on PyTorch is very useful.\n","\n","Knowing a little bit about the [transformers](https://github.com/huggingface/transformers) and [datasets](https://github.com/huggingface/datasets) help too.\n","\n","In this notebook, **I am using supervised data to pre-train / fine-tune Transformer-based models** though there is no need for labeled data in the pre-training.\n","\n","<br>\n","\n","## **How to use this notebook?**\n","\n","\n","This notebook pulls the dataset directly from [HuggingFace Hub](https://huggingface.co/datasets). People can also use this notebook with other datasets from the Hub or loading from local (though you should modify some lines to make it work in this case).\n","\n","All parameters that can be changed are under the **Parameters Setup** section. Each parameter is nicely commented and structured to be as intuitive as possible.\n","\n","<br>\n","\n","\n","## **What transformers models work with this notebook?**\n","\n","This notebook is tested to work with BERT/RoBERTa. However, it should work with more types of transformers as well. People who want to use other architectures should also check the `tokenizer` and modify the appropriate parameters to match with those architectures.\n","\n","\n","<br>\n","\n","## **Dataset**\n","\n","This notebook will cover pre-training Transformer-based model on a dataset. I will use the Vietnamese-translated PubMed dataset [ViPubMed](https://huggingface.co/datasets/VietAI/vi_pubmed) and the Vietnamese Medical Natural Language Inference dataset [ViMedNLI](https://github.com/vietai/ViPubmed/tree/main/data/vi_mednli), which was released by VietAI Research team.\n","\n","**Why this dataset?** Due to its size and simplicity, I believe that this dataset is easy to understand/use for classification and we can have more time to play with the model instead of waiting for the model training.\n","<br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UCLtm5BiXona"},"source":["## **Install Dependencies**\n","\n","* **[transformers](https://github.com/huggingface/transformers)** library needs to be installed to use and train Transformer-based models from HuggingFace.\n","\n","* **[datasets](https://github.com/huggingface/datasets)** library needs to be installed to pull public datasets from HuggingFace Hub.\n","\n","* **[tokenizers](https://github.com/huggingface/tokenizers)** library needs to be installed to train a new tokenizer.\n","\n","* **[sentencepiece](https://github.com/google/sentencepiece)** library needs to be installed to train a new tokenizer.\n","\n","* **[accelerate](https://github.com/huggingface/accelerate)** library needs to be installed to train transformers effectively and efficiently.\n"]},{"cell_type":"code","metadata":{"id":"1JQhmThRXp7b"},"source":["# Install the latest version.\n","!pip install transformers datasets tokenizers sentencepiece accelerate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5IO8-xrXvWY"},"source":["## **Imports**\n","\n","Import all needed libraries for this notebook.\n","\n","Declare basic parameters used for this notebook:\n","\n","* `set_seed(69)` - Always good to set a fixed seed for reproducibility.\n","\n","* `device` - Look for gpu to use. I will use cpu by default if no gpu found."]},{"cell_type":"code","metadata":{"id":"ux2sgMCvq8PE"},"source":["import io\n","import os\n","import math\n","import torch\n","import warnings\n","from itertools import chain\n","from dataclasses import dataclass\n","from tqdm.notebook import tqdm\n","from collections.abc import Mapping\n","from datasets import load_dataset\n","from torch.utils.data.dataset import Dataset\n","from transformers.data.data_collator import DataCollatorMixin\n","from transformers import (CONFIG_MAPPING,\n","                          MODEL_FOR_MASKED_LM_MAPPING,\n","                          PreTrainedTokenizer,\n","                          TrainingArguments,\n","                          AutoConfig,\n","                          AutoTokenizer,\n","                          AutoModelForMaskedLM,\n","                          DataCollatorForLanguageModeling,\n","                          DataCollatorForWholeWordMask,\n","                          PretrainedConfig,\n","                          Trainer,\n","                          set_seed)\n","\n","# Set seed for reproducibility,\n","set_seed(69)\n","\n","# Look for gpu to use. Will use `cpu` by default if no gpu found.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdDFhC762xRm"},"source":["## **Helper Functions**\n","\n","All classes and functions that will be used in this notebook are kept under this section to help maintain a clean look of the notebook:\n","\n","**ModelDataArguments**\n","\n","This class follows similar format as the [transformers]((https://github.com/huggingface/transformers) library. The main difference is the way I combined multiple types of arguments into one and used rules to make sure the arguments used are correctly set. Here are all argument details (they are also mentioned in the documentation):\n","\n","* `model_type`:\n","  *Type of model used: bert, roberta.\n","  More details [here](https://huggingface.co/transformers/pretrained_models.html).*\n","\n","* `config_name`:\n","  *Config of model used: bert, roberta.\n","  More details [here](https://huggingface.co/transformers/pretrained_models.html).*\n","\n","* `tokenizer_name`:\n","  *Tokenizer used to process data for training the model.\n","  It usually has same name as `model_name_or_path`: bert-base-cased,\n","  roberta-base etc.*\n","\n","* `model_name_or_path`:\n","  *Path to existing transformers model or name of\n","  transformer model to be used: bert-base-cased, roberta-base etc.\n","  More details [here](https://huggingface.co/transformers/pretrained_models.html).*\n","\n","* `dataset_name`:\n","  *The name of the dataset to use (via the datasets library).*\n","\n","* `dataset_config_name`:\n","  *The configuration name of the dataset to use (via the datasets library).*\n","\n","* `train_file`:\n","  *The input training data file.*\n","\n","* `validation_file`:\n","  *An optional input evaluation data file to evaluate the performance on.*\n","\n","* `test_file`:\n","  *An optional input test data file evaluate the performance on.*\n","\n","* `cache_dir`:\n","  *Path to cache files. It helps to save time when re-running code.*\n","\n","* `preprocessing_num_workers`:\n","  *The number of processes to use for the preprocessing.*\n","\n","* `line_by_line`:\n","  *Whether distinct lines of text in the dataset are to be handled as distinct sequences.*\n","\n","* `whole_word_mask`:\n","  *Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words.*\n","\n","* `mlm_probability`:\n","  *Used when training masked language models. Needs to have `mlm=True`.\n","  It represents the probability of masking tokens when training model.*\n","\n","* `max_seq_length`:\n","  *The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.*\n","\n","* `pad_to_max_length`:\n","  *Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when batching to the maximum length in the batch.*\n","\n","* `overwrite_cache`:\n","  *If there are any cached files, overwrite them.*\n","\n","* `validation_split_percentage`:\n","  *The percentage of the train set used as validation set in case there's no validation split.*\n","\n"]},{"cell_type":"code","metadata":{"id":"r2DABVOmwC_J"},"source":["class ModelDataArguments(object):\n","  r\"\"\"Arguments pertaining to which model/config/tokenizer/data we are going to fine-tune, or train.\n","\n","  Eve though all arguments are optional, there still needs to be a certain\n","  number of arguments that require values attributed.\n","\n","  Raises:\n","\n","        ValueError: If `CONFIG_MAPPING` is not loaded in global variables.\n","\n","        ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`.\n","\n","        ValueError: If `model_type`, `model_config_name` and\n","          `model_name_or_path` variables are all `None`. At least one of them\n","          needs to be set.\n","\n","        warnings: If `model_config_name` and `model_name_or_path` are both\n","          `None`, the model will be trained from scratch.\n","\n","  \"\"\"\n","\n","  def __init__(self,\n","               model_type=None,\n","               config_name=None,\n","               tokenizer_name=None,\n","               model_name_or_path=None,\n","               dataset_name=None,\n","               dataset_config_name=None,\n","               train_file=None,\n","               validation_file=None,\n","               test_file=None,\n","               cache_dir=None,\n","               preprocessing_num_workers=None,\n","               line_by_line=False,\n","               whole_word_mask=False,\n","               mlm_probability=0.15,\n","               max_seq_length=-1,\n","               pad_to_max_length=False,\n","               overwrite_cache=False,\n","               validation_split_percentage=5,\n","               ):\n","\n","    # Make sure CONFIG_MAPPING is imported from transformers module.\n","    if 'CONFIG_MAPPING' not in globals():\n","      raise ValueError('Could not find CONFIG_MAPPING imported! ' \\\n","                       'Make sure to import it from `transformers` module!')\n","\n","    # Make sure model_type is valid.\n","    if (model_type is not None) and (model_type not in CONFIG_MAPPING.keys()):\n","      raise ValueError('Invalid `model_type`! Use one of the following: %s' %\n","                       (str(list(CONFIG_MAPPING.keys()))))\n","\n","    # Make sure that model_type, config_name and model_name_or_path\n","    # variables are not all `None`.\n","    if not any([model_type, config_name, model_name_or_path]):\n","      raise ValueError('You can`t have all `model_type`, `config_name`,' \\\n","                       ' `model_name_or_path` be `None`! You need to have' \\\n","                       'at least one of them set!')\n","\n","    # Check if a new model will be loaded from scratch.\n","    if not any([config_name, model_name_or_path]):\n","      # Setup warning to show pretty. This is an overkill\n","      warnings.formatwarning = lambda message,category,*args,**kwargs: \\\n","                               '%s: %s\\n' % (category.__name__, message)\n","      # Display warning.\n","      warnings.warn('You are planning to train a model from scratch! üôÄ')\n","\n","    # Set all data related arguments.\n","    self.dataset_name = dataset_name\n","    self.dataset_config_name = dataset_config_name\n","    self.train_file = train_file\n","    self.validation_file = validation_file\n","    self.test_file = test_file\n","    self.preprocessing_num_workers = preprocessing_num_workers\n","    self.line_by_line = line_by_line\n","    self.whole_word_mask = whole_word_mask\n","    self.mlm_probability = mlm_probability\n","    self.max_seq_length = max_seq_length\n","    self.pad_to_max_length = pad_to_max_length\n","    self.overwrite_cache = overwrite_cache\n","    self.validation_split_percentage = validation_split_percentage\n","\n","    # Set all model and tokenizer arguments.\n","    self.model_type = model_type\n","    self.config_name = config_name\n","    self.tokenizer_name = tokenizer_name\n","    self.model_name_or_path = model_name_or_path\n","    self.cache_dir = cache_dir\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Config\n","\n","Model configuration defines a Transformer-based language model architecture with a configurable number of layers, hidden size, embedding size, and etc. These settings significantly impact the number of model parameters and therefore affect the model's capacity to learn and perform well on downstream tasks.\n","\n","_function_ **get_model_config(args: ModelDataArguments, override_config)**\n","\n","Using the ModelDataArguments to return the model configuration. Here are all argument detailed:\n","\n","* `args`: *Model and data configuration arguments needed to perform pretraining.*\n","\n","* `override_config`: *Configuration to replace the old one.*\n","\n","* Returns: *Model transformers configuration.*\n"],"metadata":{"id":"6ZcosHrFILW3"}},{"cell_type":"code","source":["def get_model_config(args: ModelDataArguments, override_config):\n","  r\"\"\"\n","  Get model configuration.\n","\n","  Using the ModelDataArguments return the model configuration.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pretraining.\n","\n","    override_config (:obj:`Config`):\n","      Configuration to replace the old one.\n","\n","  Returns:\n","\n","    :obj:`PretrainedConfig`: Model transformers configuration.\n","\n","  \"\"\"\n","\n","  # Check model configuration.\n","  if args.config_name is not None:\n","    # Use model configure name if defined.\n","    model_config = AutoConfig.from_pretrained(args.config_name,\n","                                      cache_dir=args.cache_dir)\n","\n","  elif args.model_name_or_path is not None:\n","    # Use model name or path if defined.\n","    model_config = AutoConfig.from_pretrained(args.model_name_or_path,\n","                                      cache_dir=args.cache_dir)\n","\n","  else:\n","    # Use config mapping if building model from scratch.\n","    model_config = CONFIG_MAPPING[args.model_type]()\n","\n","  if override_config:\n","    model_config.update(override_config)\n","\n","  return model_config\n"],"metadata":{"id":"ZG_wdQL-IOKa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenizer\n","\n","The function is responsible for returning a tokenizer object that can be used to preprocess raw text data for training the pre-training model. We will load our trained tokenizer in the previous section.\n","\n","_function_ **get_tokenizer(args: ModelDataArguments, local_dir, config)**\n","\n","Using the ModelDataArguments return the model tokenizer and change `max_seq_length` from `args` if needed. Here are all argument detailed:\n","\n","* `args`: *Model and data configuration arugments needed to perform pretraining.*\n","\n","* `local_dir`: *Path to the trained tokenizer.*\n","\n","* `config`: *Model Configuration.*\n","\n","* Returns: *Model transformers tokenizer.*"],"metadata":{"id":"xZOgBFjcIC40"}},{"cell_type":"code","source":["def get_tokenizer(args: ModelDataArguments, local_path, config):\n","  r\"\"\"\n","  Get model tokenizer.\n","\n","  Using the ModelDataArguments return the model tokenizer and change\n","  `max_seq_length` from `args` if needed.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pre-training.\n","\n","    local_path (:obj:`str`):\n","      Path to the trained tokenizer.\n","\n","    config (:obj:`Config`):\n","      Model Configuration.\n","\n","  Returns:\n","\n","    :obj:`PreTrainedTokenizer`: Model transformers tokenizer.\n","\n","  \"\"\"\n","\n","  # Check tokenizer configuration.\n","  if args.tokenizer_name:\n","    # Use tokenizer name if defined.\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name,\n","                                              cache_dir=args.cache_dir)\n","\n","  elif args.model_name_or_path:\n","    # Use tokenizer name of path if defined.\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\n","                                              cache_dir=args.cache_dir)\n","  else:\n","    tokenizer = AutoTokenizer.from_pretrained(local_path,\n","                                              config=config,\n","                                              cache_dir=args.cache_dir)\n","\n","  # Setup data maximum number of tokens.\n","  if args.max_seq_length <= 0:\n","    # Set max_seq_length to maximum length of tokenizer.\n","    # Input max_seq_length will be the max possible for the model.\n","    args.max_seq_length = tokenizer.model_max_length\n","  else:\n","    # Never go beyond tokenizer maximum length.\n","    args.max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n","\n","  return tokenizer\n"],"metadata":{"id":"d0dffc_4IEUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mask Language Models\n","\n","This notebook only focuses on training Masked language models such as BERT, RoBERTa. Therefore, we will use the `AutoModelForMaskedLM` class to initialize the model.\n","\n","\n","\n","_funcion_ **get_model(args: ModelDataArguments, model_config)**\n","\n","Using the ModelDataArguments return the actual model. Here are all argument detailed:\n","\n","* `args`: *Model and data configuration arguments needed to perform pretraining.*\n","\n","* `model_config`: *Model transformers configuration.*\n","\n","* Returns: *transformers model object.*"],"metadata":{"id":"V7TU4Ek6H2Yz"}},{"cell_type":"code","source":["def get_model(args: ModelDataArguments, model_config):\n","  r\"\"\"\n","  Get model.\n","\n","  Using the ModelDataArguments return the actual model.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pretraining.\n","\n","    model_config (:obj:`PretrainedConfig`):\n","      Model transformers configuration.\n","\n","  Returns:\n","\n","    :obj:`torch.nn.Module`: PyTorch model.\n","\n","  \"\"\"\n","\n","  # Make sure MODEL_FOR_MASKED_LM_MAPPING is imported from transformers module.\n","  if 'MODEL_FOR_MASKED_LM_MAPPING' not in globals():\n","    raise ValueError('Could not find MODEL_FOR_MASKED_LM_MAPPING is imported!' \\\n","                     ' Make sure to import them from `transformers` module!')\n","\n","  # Check if using pre-trained model or train from scratch.\n","  if args.model_name_or_path:\n","    # Use pre-trained model.\n","    if type(model_config) in MODEL_FOR_MASKED_LM_MAPPING.keys():\n","      # Masked language modeling head.\n","      return AutoModelForMaskedLM.from_pretrained(\n","                        args.model_name_or_path,\n","                        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","                        config=model_config,\n","                        cache_dir=args.cache_dir,\n","                        )\n","    else:\n","      raise ValueError(\n","          'Invalid `model_name_or_path`! It should be in %s or %s!' %\n","          str(MODEL_FOR_MASKED_LM_MAPPING.keys())\n","        )\n","\n","  else:\n","      # Use model from configuration - train from scratch.\n","      print(\"Training new model from scratch!\")\n","      return AutoModelForMaskedLM.from_config(config)\n"],"metadata":{"id":"8gxs2jFzH4rb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset & Preprocessing\n","\n","There are two functions in this subsection: the first is to load data from Github using HF `datasets` library, and second, is to tokenize the text data into lines or chunks.\n","\n","_function_ **get_dataset(args: ModelDataArguments)**\n","\n","Get the raw datasets from Hugging Face. Here are all argument detailed:\n","\n","* `args`: *Model and data configuration arguments needed to perform pretraining.*\n","\n","* Returns: *Dataset.*\n","\n","<br>\n","\n","_function_ **preprocess_data(args: ModelDataArguments, train_args: TrainingArguments, dataset: Dataset, tokenizer: PreTrainedTokenizer)**\n","\n","  Preprocess and tokenize the dataset.\n","\n","  This function can tokenize each nonempty line in the dataset or group chunks together\n","  after tokenizing every text.\n","\n","  Arguments:\n","\n","* `args`: Model and data configuration arguments needed to perform pretraining.\n","\n","* `train_args`: Training arguments needed to perform pretraining.\n","\n","* `dataset`: Raw dataset that needs to be preprocessed.\n","\n","* `tokenizer`: Model transformers tokenizer.\n","\n","* Returns: *Tokenized Dataset.*"],"metadata":{"id":"y8IAb3W7Hp8Y"}},{"cell_type":"code","source":["def get_dataset(args: ModelDataArguments):\n","  r\"\"\"\n","  Get the raw datasets from Hugging Face.\n","\n","  Using the ModelDataArguments return the actual model.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pretraining.\n","\n","  Returns:\n","\n","    :obj:`Dataset`: PyTorch dataset that contains text data.\n","\n","  \"\"\"\n","\n","  # Loading data using datasets.\n","  if args.dataset_name is not None:\n","    # Downloading and loading a dataset from the hub.\n","    raw_datasets = load_dataset(\n","        args.dataset_name,\n","        args.dataset_config_name,\n","        cache_dir=args.cache_dir,\n","    )\n","    # Splitting the dataset into train and validation set if need.\n","    if \"validation\" not in raw_datasets.keys():\n","      raw_datasets[\"validation\"] = load_dataset(\n","          args.dataset_name,\n","          args.dataset_config_name,\n","          split=f\"train[:{args.validation_split_percentage}%]\",\n","          cache_dir=args.cache_dir,\n","      )\n","      raw_datasets[\"train\"] = load_dataset(\n","          args.dataset_name,\n","          args.dataset_config_name,\n","          split=f\"train[{args.validation_split_percentage}%:]\",\n","          cache_dir=args.cache_dir,\n","      )\n","  else:\n","    data_files = {}\n","    if args.train_file is not None:\n","        data_files[\"train\"] = args.train_file\n","        extension = args.train_file.split(\".\")[-1]\n","\n","    if args.validation_file is not None:\n","        data_files[\"validation\"] = args.validation_file\n","        extension = args.validation_file.split(\".\")[-1]\n","    if args.test_file is not None:\n","        data_files[\"test\"] = args.test_file\n","        extension = args.test_file.split(\".\")[-1]\n","    raw_datasets = load_dataset(\n","        extension,\n","        data_files=data_files,\n","        field=\"data\",\n","        cache_dir=args.cache_dir,\n","    )\n","\n","    # Splitting the dataset into train and validation set if need.\n","    if \"validation\" not in raw_datasets.keys():\n","      raw_datasets[\"validation\"] = load_dataset(\n","          extension,\n","          data_files=data_files,\n","          split=f\"train[:{args.validation_split_percentage}%]\",\n","          cache_dir=args.cache_dir,\n","      )\n","      raw_datasets[\"train\"] = load_dataset(\n","          extension,\n","          data_files=data_files,\n","          split=f\"train[{args.validation_split_percentage}%:]\",\n","          cache_dir=args.cache_dir,\n","      )\n","\n","  return raw_datasets\n","\n","\n","def preprocess_data(args: ModelDataArguments, train_args: TrainingArguments, dataset: Dataset, tokenizer: PreTrainedTokenizer):\n","  r\"\"\"\n","  Preprocess and tokenize the dataset.\n","\n","  1. This function can tokenize each nonempty line in the dataset for pretraining.\n","  2. This function can tokenize sentence pairs for finetuning NLI task.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pretraining.\n","\n","    train_args (:obj:`TrainingArguments`):\n","      Training arguments needed to perform pretraining.\n","\n","    dataset (:obj:`Dataset`):\n","      Raw dataset that needs to be preprocessed.\n","\n","    tokenizer (:obj:`PreTrainedTokenizer`):\n","      Model transformers tokenizer.\n","\n","  Returns:\n","\n","    :obj:`Dataset`: PyTorch Dataset that contains file's data.\n","\n","  \"\"\"\n","\n","  padding = \"max_length\" if args.pad_to_max_length else False\n","\n","  if train_args.do_train:\n","      column_names = list(dataset[\"train\"].features)\n","  else:\n","      column_names = list(dataset[\"validation\"].features)\n","\n","  # Preprocessing the datasets.\n","  # First we tokenize all the texts.\n","  if args.line_by_line:\n","    # When using line_by_line, we just tokenize each nonempty line.\n","    # TODO: Implement a function to tokenize a batch of inputs\n","    pass\n","  else:\n","    # TODO: Implement data processing for text classification\n","    pass\n","\n","  # TODO: Execute tokenization using datasets map()\n","\n","  # Return tokenized datasets\n","  return tokenized_datasets"],"metadata":{"id":"e-thu_oAHpQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Collator\n","\n","Data collator is used to combine a batch of input examples into a single input tensor that can be processed by a neural network. The data collator is typically used in conjunction with a `dataloader`, which batches individual input examples together and passes them to the collator. The specific behavior of the data collator can vary depending on the task and the model being used. In this notebook, we will use DataCollator to randomly mask the tokens.\n","\n","_function_ **get_collator(args: ModelDataArguments, model_config: PretrainedConfig, tokenizer: PreTrainedTokenizer)**\n","\n","Collator function will be used to collate a PyTorch Dataset object. Here are all argument detailed:\n","\n","* `args`: *Model and data configuration arguments needed to perform pretraining.*\n","\n","* `tokenizer`: *Model transformers tokenizer.*\n","\n","* Returns: *Transformers specific data collator.*"],"metadata":{"id":"M7M0a9SJEXdS"}},{"cell_type":"code","source":["def get_collator(args: ModelDataArguments, tokenizer: PreTrainedTokenizer):\n","  r\"\"\"\n","  Get appropriate collator function.\n","\n","  Collator function will be used to collate a PyTorch Dataset object.\n","\n","  Arguments:\n","\n","    args (:obj:`ModelDataArguments`):\n","      Model and data configuration arguments needed to perform pretraining.\n","\n","    tokenizer (:obj:`PreTrainedTokenizer`):\n","      Model transformers tokenizer.\n","\n","  Returns:\n","\n","    :obj:`data_collator`: Transformers specific data collator.\n","\n","  \"\"\"\n","\n","  # Special dataset handle depending on model type.\n","  if args.whole_word_mask:\n","    # Use whole word masking.\n","    return DataCollatorForWholeWordMask(\n","                                        tokenizer=tokenizer,\n","                                        mlm_probability=args.mlm_probability,\n","                                        )\n","  else:\n","    # Regular language modeling.\n","    return CustomDataCollatorForLanguageModeling(\n","                                        tokenizer=tokenizer,\n","                                        mlm=True,\n","                                        mlm_probability=args.mlm_probability,\n","                                        )"],"metadata":{"id":"aHaKaiEGEWyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Re-implementing the masking strategy is a useful exercise to gain a deeper understanding of this important technique. While `transformers` library provides a well-implemented function, there is still value in practicing the implementation ourselves. By doing so, we can gain insights into the nuances of masking and further refine our pre-training strategies. With this knowledge, you can create customized pre-training strategies to suit your specific NLP tasks."],"metadata":{"id":"EBd3RWCqdO4U"}},{"cell_type":"code","source":["@dataclass\n","class CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n","    \"\"\"\n","    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n","    are not all of the same length.\n","\n","    Args:\n","        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n","            The tokenizer used for encoding the data.\n","        mlm (`bool`, *optional*, defaults to `True`):\n","            Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs\n","            with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked\n","            tokens and the value to predict for the masked token.\n","        mlm_probability (`float`, *optional*, defaults to 0.15):\n","            The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n","        pad_to_multiple_of (`int`, *optional*):\n","            If set will pad the sequence to a multiple of the provided value.\n","        return_tensors (`str`):\n","            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n","\n","    <Tip>\n","\n","    For best performance, this data collator should be used with a dataset having items that are dictionaries or\n","    BatchEncoding, with the `\"special_tokens_mask\"` key, as returned by a [`PreTrainedTokenizer`] or a\n","    [`PreTrainedTokenizerFast`] with the argument `return_special_tokens_mask=True`.\n","\n","    </Tip>\"\"\"\n","\n","    tokenizer: PreTrainedTokenizer\n","    mlm: bool = True\n","    mlm_probability: float = 0.15\n","    pad_to_multiple_of: int = None\n","    return_tensors: str = \"pt\"\n","\n","    def __post_init__(self):\n","        if self.mlm and self.tokenizer.mask_token is None:\n","            raise ValueError(\n","                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n","                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n","            )\n","\n","    def torch_call(self, examples):\n","        # Padding examples to the same length.\n","        batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n","\n","        # If special token mask has been preprocessed, pop it from the dict.\n","        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n","        if self.mlm:\n","            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n","                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n","            )\n","        else:\n","            labels = batch[\"input_ids\"].clone()\n","            if self.tokenizer.pad_token_id is not None:\n","                labels[labels == self.tokenizer.pad_token_id] = -100\n","            batch[\"labels\"] = labels\n","        return batch\n","\n","    def torch_mask_tokens(self, inputs, special_tokens_mask = None):\n","        \"\"\"\n","        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n","        \"\"\"\n","        import torch\n","\n","        labels = inputs.clone()\n","        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n","        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n","        if special_tokens_mask is None:\n","            special_tokens_mask = [\n","                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n","            ]\n","            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n","        else:\n","            special_tokens_mask = special_tokens_mask.bool()\n","\n","        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n","        masked_indices = torch.bernoulli(probability_matrix).bool()\n","        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n","\n","        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n","        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n","        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n","\n","        # 10% of the time, we replace masked input tokens with random word\n","        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n","        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n","        inputs[indices_random] = random_words[indices_random]\n","\n","        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n","        return inputs, labels\n"],"metadata":{"id":"BwoX8LqacPt8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define model arguments\n","\n","Before diving deeper, we need to define some important arguments based on our needs.\n","\n","First, declare some neccessary arguments used in the next steps:\n","\n","* `output_model_dir` - The output directory where the tokenizer and model checkpoints will be written.\n","\n","* `model_type` - The type of the model i.g., bert, roberta, etc.\n","\n","* `dataset_name` - The name of the dataset from Hugging Face.\n","\n","* `vocab_size` - The size of the vocabulary.\n","\n","* `max_seq_length` - The maximum number of sequence.\n","\n","* `mlm_probability` - The probability of masking tokens.\n","\n","* `whole_word_mask` - Whether to mask the entire word.\n","\n","* `line_by_line` - Whether to tokenize each nonempty line."],"metadata":{"id":"UhoyYp38SI-7"}},{"cell_type":"code","source":["output_model_dir = \"./mlm_bert\"  # @param {type:\"string\"}\n","\n","model_type = 'roberta'  # @param {type:\"string\"}\n","\n","datatset_name = 'razent/vi_pubmed_small'  # @param {type:\"string\"}\n","\n","# Let‚Äôs arbitrarily pick its size to be 5,000.\n","vocab_size = 4000   # @param {type:\"number\"}\n","\n","max_seq_length = 128   # @param {type:\"number\"}\n","\n","mlm_probability = 0.15   # @param {type:\"number\"}\n","\n","whole_word_mask = False   # @param {type:\"boolean\"}\n","\n","line_by_line = True   # @param {type:\"boolean\"}\n","\n","pad_to_max_length = True   # @param {type:\"boolean\"}\n","\n","# Create model folder\n","if not os.path.exists(output_model_dir):\n","  !mkdir $output_model_dir"],"metadata":{"id":"RQqiOL9kVz2N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As I mentioned in the first section, we will use the [ViPubMed](https://huggingface.co/datasets/razent/vi_pubmed_small) dataset for pretraining part. Since we want to pre-training RoBERTa model from scratch, we only need to set the `model_type` to `roberta` (just ignore `config_name`, `model_name_or_path`). We just use the same `mlm_probability=0.15` as BERT and set the maximum number of sequences to `128` for faster training."],"metadata":{"id":"-x7py5kpWwIm"}},{"cell_type":"code","source":["# Define arguments for data, tokenizer and model arguments.\n","# See comments in `ModelDataArguments` class.\n","model_data_args = ModelDataArguments(\n","                                    dataset_name=datatset_name,\n","                                    line_by_line=line_by_line,\n","                                    whole_word_mask=whole_word_mask,\n","                                    mlm_probability=mlm_probability,\n","                                    max_seq_length=max_seq_length,\n","                                    pad_to_max_length=pad_to_max_length,\n","                                    overwrite_cache=False,\n","                                    model_type=model_type,\n","                                    cache_dir=None\n","                                    )"],"metadata":{"id":"JiXHh1SrSIIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pulling the dataset\n","\n","You're right, we first need to get the dataset before going further."],"metadata":{"id":"W3M-Rg3wOjOC"}},{"cell_type":"code","source":["# Setup train dataset if `do_train` is set.\n","print('Pulling the dataset...')\n","datasets = get_dataset(model_data_args)\n","datasets"],"metadata":{"id":"5f-eBzX-ynP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset that we are working with is composed of a total of 5,000 samples, which have been partitioned into two subsets: 4,750 samples for training, 250 samples for validation. Each of these samples includes a single column, namely a `text` column. With such a comprehensive dataset, we can confidently train and evaluate our models to achieve considerable performance in Medical Natural Language Inference.\n","\n","Now that we have our corpus in the form of an iterator of texts, we are ready to train a new tokenizer.\n"],"metadata":{"id":"xKEsrxQ1VqIn"}},{"cell_type":"markdown","source":["## Train a tokenizer\n","\n","In this section, we will train a byte-level Byte-Pair Encoding (BPE) tokenizer from scratch.\n","\n","We recommend training a byte-level BPE because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!). So we don‚Äôt need to specify an `unk_token`.\n","\n","Before the tokenizer splits a text into subtokens, it will need to perform some preprocessing steps.\n","\n","Here's a high-level overview of the steps in the tokenization pipeline:\n","\n","![Img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg)\n","\n","Fortunately, the ü§ó Tokenizers library has been built to provide several options for each of those steps, so we just need to read the documentation and mix/match them together.\n","\n","The full pipeline steps are:\n","- Normalization (`normalizers`): any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc. [Read more](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)\n","- Pre-tokenization (`pre_tokenizers`): splitting the input into words. [Read more](https://huggingface.co/docs/tokenizers/api/tokenizer#module-tokenizers.pre_tokenizers)\n","- Model (`models`): running the input through the model (using the pre-tokenized words to produce a sequence of tokens). [Read more](https://huggingface.co/docs/tokenizers/api/tokenizer#module-tokenizers.models)\n","- Post-processing (`post_processors`): adding the special tokens of the tokenizer, generating the attention mask and token type IDs. [Read more](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)\n","+ The library provides `decoders` various types of Decoder for decoding the outputs of tokenization. [Read more](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)\n","\n","_‚ö†Ô∏è Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It‚Äôs randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It‚Äôs deterministic, meaning you always get the same results when training with the same algorithm on the same corpus._"],"metadata":{"id":"5iZNbCS6nJD2"}},{"cell_type":"code","source":["from tokenizers import models, pre_tokenizers, decoders, trainers, processors, Tokenizer, ByteLevelBPETokenizer\n","\n","# Initialize a tokenizer\n","tokenizer = Tokenizer(models.BPE())   # or use ByteLevelBPETokenizer()\n","tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) # Since RoBERTa does not use a normalizer, so we skip that step and go directly to the pre-tokenization.\n"],"metadata":{"id":"leqDRvcvVFDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Though, we could initialize this model with a vocabulary if we had one (we would need to pass the `vocab.json` and `merges.txt` in this case), since we will train from scratch, we don't need to do that.\n","\n","The option we added to ByteLevel here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before:"],"metadata":{"id":"_c3fzw71-LRP"}},{"cell_type":"code","source":["tokenizer.pre_tokenizer.pre_tokenize_str(\"Hi, my name is Hieu!\")"],"metadata":{"id":"X-B36ZaQ_OBR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This tokenizer has a few special symbols, like ƒ† and ƒä, which denote spaces and newlines, respectively.\n","\n","Next, we will start training the model with the same special tokens as RoBERTa. We also specify the `min_frequency` that indicates the number of times a token must appear to be included in the vocabulary."],"metadata":{"id":"SXRiewkz_Z17"}},{"cell_type":"code","source":["%%time\n","# Customize training with only text data from the training dataset\n","trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[\n","    \"<s>\", #[CLS]\n","    \"<pad>\",\n","    \"</s>\",\n","    \"<unk>\",\n","    \"<mask>\",\n","])\n","tokenizer.train_from_iterator(iterator=datasets['train']['text'], trainer=trainer)\n"],"metadata":{"id":"i5ArM_O3-Hha"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•\n","\n","What great is that our tokenizer is optimized for Vietnamese. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Vietnamese ‚Äì `·ªè`, `·ªÉ`, `·ª´`, `ƒ©`, `·ª∑`, and `·∫∑` ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained RoBERTa tokenizer.\n","\n","Here‚Äôs  how you can use it in `tokenizers`, including handling the RoBERTa special tokens.\n"],"metadata":{"id":"4w7lIxziWO1r"}},{"cell_type":"code","source":["# Add special tokens at the begining and the end of a sentence.\n","tokenizer.post_processor = processors.RobertaProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","\n","tokenizer.enable_truncation(max_length=512)"],"metadata":{"id":"rFqL6H37D_M_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoding = tokenizer.encode(\"Xin ch√†o VietAI nh√©!\")\n","encoding.tokens"],"metadata":{"id":"v3RYYRcnKLS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The encoding obtained is an Encoding, which contains all the necessary outputs of the tokenizer in its various attributes: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, and `overflowing`."],"metadata":{"id":"_emQF7T9FNlA"}},{"cell_type":"markdown","source":["Finally, we add a byte-level decoder:"],"metadata":{"id":"zSolXKCNGE7j"}},{"cell_type":"code","source":["tokenizer.decoder = decoders.ByteLevel()"],"metadata":{"id":"bQSKc67gGF3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and we can double-check it works properly:"],"metadata":{"id":"kRDbdRsoGMMh"}},{"cell_type":"code","source":["tokenizer.decode(encoding.ids)"],"metadata":{"id":"O5Njd5b2GH-b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great! Now that we‚Äôre done, we can save the tokenizer, and wrap it in a `PreTrainedTokenizerFast` or other tokenizer supported in `transformers`.\n","\n","For example:\n","```\n","RobertaTokenizerFast(tokenizer_object=tokenizer)\n","```"],"metadata":{"id":"Q0NjPKrsGWNY"}},{"cell_type":"code","source":["# Save files to disk\n","tokenizer.save(f'{output_model_dir}/tokenizer.json')"],"metadata":{"id":"ExJVmwBOMV_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now have a `tokenizer.json`, which is a list of the most frequent tokens ranked by frequency. Note that, this file will be loaded as TokenizerFast later.\n","\n","\n","```json\n","{\n","  \"<s>\": 0,\n","\t\"<pad>\": 1,\n","\t\"</s>\": 2,\n","\t\"<unk>\": 3,\n","\t\"<mask>\": 4,\n","\t\"!\": 5,\n","\t\"\\\"\": 6,\n","\t# ...\n","}\n","```"],"metadata":{"id":"ChkJelvDNAFW"}},{"cell_type":"markdown","metadata":{"id":"E1F-XIQCdOgj"},"source":["## **Parameters Setup**\n","\n","Declare the rest of the parameters used for this notebook:\n","\n","* `model_data_args` contains all arguments needed to setup dataset, model configuration, model tokenizer and the actual model. This is created using the `ModelDataArguments` class.\n","\n","* `training_args` contains all arguments needed to use the [Trainer](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainer) functionality from Transformers that allows us to train transformers models in PyTorch very easy. You can find the complete documentation [here](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). There are a lot of parameters that can be set to allow multiple functionalities:\n","\n","  * `output_dir`: *The output directory where the model predictions and checkpoints will be written. I set it up to `output_model_dir` where the model and will be saved.*\n","\n","  * `overwrite_output_dir`: *Overwrite the content of the output directory. I set it to `True` in case I run the notebook multiple times and I only care about the last run.*\n","\n","  * `do_train`: *Whether to run training or not. I set this parameter to `True` because I want to train the model on my own dataset.*\n","\n","  * `do_eval`: *Whether to run evaluation or not.\n","I set it to `True` since I have test data file and I want to evaluate how well the model trains.*\n","\n","  * `per_device_train_batch_size`: *Batch size GPU/TPU core/CPU training. I set it to `2` for this example. I recommend setting it up as high as your GPU memory allows you.*\n","\n","  * `per_device_eval_batch_size`: *Batch size  GPU/TPU core/CPU for evaluation.*\n","\n","  * `evaluation_strategy`: *Evaluation strategy to adopt during training:*\n","    - `no`: *No evaluation during training;*\n","    \n","    - `steps`: *Evaluate every `eval_steps;*\n","    \n","    - `epoch`: *Evaluate every end of epoch. I set it to 'steps' since I want to evaluate model more often.*\n","\n","  * `logging_steps`: *How often to show logs. I set this to `500` just as an example. If your evaluate data is large you might not want to run it that often because it will significantly slow down training time.*\n","\n","  * `eval_steps`: *Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set. Since I want to evaluate model ever`logging_steps` I will set this to `None` since it will inherit same value as `logging_steps`.*\n","\n","  * `prediction_loss_only`: *Set prediction loss to `True` in order to return loss for perplexity calculation.*\n","\n","  * `learning_rate`: *The initial learning rate for Adam. Defaults is set to `5e-5`.*\n","\n","  * `weight_decay`: *The weight decay to apply (if not zero). Defaults is set to `0`.*\n","\n","  * `adam_epsilon`: *Epsilon for the Adam optimizer. Defaults to `1e-8`.*\n","\n","  * `max_grad_norm`: *Maximum gradient norm (for gradient clipping). Defaults to `0`.*\n","\n","  * `num_train_epochs`: *Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). I set it to `3` at most. Since the custom dataset will be a lot smaller than the original dataset the model was trained on we don't want to overfit.*\n","\n","  * `save_steps`: *Number of updates steps before two checkpoint saves. Defaults to `500`.*"]},{"cell_type":"code","source":["# Batch size GPU/TPU core/CPU training.\n","batch_size = 64  # @param {type:\"number\"}\n","\n","# The initial learning rate for Adam. Defaults to 5e-5.\n","learning_rate = 1e-4  # @param {type:\"number\"}\n","\n","# Total number of training epochs to perform (if not an integer, will perform the\n","# decimal part percents of the last epoch before stopping training). max_steps = 200_000,\n","num_train_epochs = 20  # @param {type:\"number\"}\n","\n","# How often to show logs. I will se this to plot history loss and calculate perplexity.\n","logging_steps = 100  # @param {type:\"number\"}\n","\n","# Number of updates steps before two checkpoint saves. Defaults to 500\n","save_steps = 100  # @param {type:\"number\"}\n","\n","# The weight decay to apply (if not zero).\n","weight_decay = 0.0  # @param {type:\"number\"}\n","\n","# Epsilon for the Adam optimizer. Defaults to 1e-8\n","adam_epsilon = 1e-8  # @param {type:\"number\"}\n","\n","# Maximum gradient norm (for gradient clipping). Defaults to 0.\n","max_grad_norm = 1.0  # @param {type:\"number\"}\n","\n","# The total number of saved models.\n","save_total_limit = 3  # @param {type:\"number\"}\n","\n","# Set prediction loss to `True` in order to return loss for perplexity calculation.\n","prediction_loss_only = True # @param {type: \"boolean\"}\n","\n","# Whether to run training or not.\n","do_train = True # @param {type: \"boolean\"}\n","\n","# Whether to run evaluation or not.\n","do_eval = True # @param {type: \"boolean\"}\n","\n","# Overwrite the content of the output directory.\n","overwrite_output_dir = True # @param {type: \"boolean\"}\n","\n","\n","# Define arguments for training\n","# `TrainingArguments` contains a lot more arguments.\n","# For more details check the awesome documentation:\n","# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n","training_args = TrainingArguments(\n","                          output_dir=output_model_dir,\n","                          overwrite_output_dir=overwrite_output_dir,\n","                          do_train=do_train,\n","                          do_eval=do_eval,\n","                          per_device_train_batch_size=batch_size,\n","                          logging_steps=logging_steps,\n","                          prediction_loss_only=prediction_loss_only,\n","                          learning_rate = learning_rate,\n","                          weight_decay=weight_decay,\n","                          adam_epsilon = adam_epsilon,\n","                          max_grad_norm = max_grad_norm,\n","                          num_train_epochs = num_train_epochs,\n","                          save_steps = save_steps,\n","                          save_total_limit = save_total_limit)\n","\n"],"metadata":{"id":"p-xq0fhNYbIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OA8S0HqsdpHs"},"source":["## **Load Configuration, Tokenizer and Model**\n","\n","Loading the three essential parts of the pretrained transformers: configuration, tokenizer and model.\n","\n","Since I use the AutoClass functionality from Hugging Face, I only need to worry about the model's name as input and the rest is handled by the transformers library.\n","\n","I will be calling each three functions created in the **Helper Functions** tab that help return `config` of the model, `tokenizer` of the model and the actual PyTorch `model`.\n","\n","After `model` is loaded, it is always good practice to resize the model depending on the `tokenizer` size. This means that the tokenizer's vocabulary will be aligned with the models embedding layer. This is very useful when we have a different tokenizer that the pretrained one or we train a transformer model from scratch.\n","\n"]},{"cell_type":"code","metadata":{"id":"uyUXB374duJs"},"source":["# Load model configuration.\n","print('Loading model configuration...')\n","override_config = {\n","    \"num_hidden_layers\": 8,\n","    \"num_attention_heads\": 8,\n","    \"intermediate_size\": 2048,\n","    \"hidden_size\": 512,\n","    \"max_position_embeddings\": 130 # 128+2 for special tokens\n","}\n","config = get_model_config(model_data_args, override_config)\n","\n","# Load model tokenizer.\n","print('Loading model`s tokenizer...')\n","tokenizer = get_tokenizer(model_data_args, local_path=output_model_dir, config=config)\n","\n","# Loading model.\n","print('Loading actual model...')\n","model = get_model(model_data_args, config)\n","\n","# Resize model to fit all tokens in tokenizer.\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Number of model parameters\n","print(\"Number of model parameters:\", model.num_parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VE2MRZZhd5uM"},"source":["## **Preprocess Dataset and Load Data Collator**\n","\n","This is where I process the PyTorch Dataset and data collator objects that will be used to feed data into our model.\n","\n","I strongly recommend to use a validation set in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation set can be incorporated in train and run a final train with the whole dataset."]},{"cell_type":"code","metadata":{"id":"4mkU59HSd77l"},"source":["print('Preprocessing datasets...')\n","text_datasets = datasets.remove_columns('label')\n","tokenized_datasets = preprocess_data(model_data_args, training_args, text_datasets, tokenizer)\n","\n","# Split train/eval datasets\n","train_dataset, eval_dataset = tokenized_datasets['train'], tokenized_datasets['validation']\n","print('Training set:', len(train_dataset))\n","print('Validation set:', len(eval_dataset))\n","\n","\n","# Get data collator to modify data format depending on type of model used.\n","data_collator = get_collator(model_data_args, tokenizer)\n","\n","# Check how many logging prints you'll have. This is to avoid overflowing the\n","# notebook with a lot of prints. Display warning to user if the logging steps\n","# that will be displayed is larger than 100.\n","if (len(train_dataset) // training_args.per_device_train_batch_size \\\n","    // training_args.logging_steps * training_args.num_train_epochs) > 100:\n","  # Display warning.\n","  warnings.warn('Your `logging_steps` value will will do a lot of printing!' \\\n","                ' Consider increasing `logging_steps` to avoid overflowing' \\\n","                ' the notebook with a lot of prints!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tr-UR5ZZcC0u"},"source":["## **Train RoBERTa from scratch**\n","\n","Hugging Face was very nice to us for creating the `Trainer` class. This helps make PyTorch model training of transformers very easy! We just need to make sure we loaded the proper parameters and everything else is taking care of!\n","\n","At the end of the training, the tokenizer is saved along with the model so you can easily re-use it later or even load from Hugging Face."]},{"cell_type":"code","metadata":{"id":"RWZi6QArWR8I"},"source":["%%time\n","\n","# Initialize Trainer.\n","print('Loading `trainer`...')\n","trainer = Trainer(model=model,\n","                  args=training_args,\n","                  data_collator=data_collator,\n","                  train_dataset=train_dataset,\n","                  )\n","\n","\n","print('Start training...')\n","\n","# Setup model path if the model to train loaded from a local path.\n","model_path = (model_data_args.model_name_or_path\n","              if model_data_args.model_name_or_path is not None and\n","              os.path.isdir(model_data_args.model_name_or_path)\n","              else None\n","              )\n","\n","\n","# Run training.\n","trainer.train(model_path=model_path)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### üéâ Save final model (+ tokenizer + config) to disk"],"metadata":{"id":"3MvOCc4cBYnA"}},{"cell_type":"code","source":["# Save model.\n","trainer.save_model(training_args.output_dir)\n","\n","# For convenience, we also re-save the tokenizer to the same directory,\n","# so that you can share your model easily on huggingface.co/models =).\n","if trainer.is_world_process_zero():\n","  tokenizer.save_pretrained(training_args.output_dir)"],"metadata":{"id":"BEzBUyn9BWnr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check that the LM actually trained"],"metadata":{"id":"KjE2swnJe2YN"}},{"cell_type":"markdown","source":["Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n","\n","Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n","\n"],"metadata":{"id":"gc4urVJGkmq3"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=output_model_dir,\n","    tokenizer=output_model_dir\n",")\n","\n","fill_mask(\"Android nh·∫≠p li·ªáu b·∫±ng l·ªùi n√≥i r·∫•t chu·∫©n, nhanh, nhi·ªÅu ng√¥n <mask> c√πng l√∫c.\")"],"metadata":{"id":"3fccvnMNfKOh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7vzFHOkLXKAp"},"source":["#**Finetuning**\n"]},{"cell_type":"markdown","source":["In the finetuning phase, we will need to import the `AutoModelForSequenceClassification` class because we plan to fine-tune our pre-trained model on [**ViMedNLI**](VietAI/vi_mednli), which is one of the text classification problems."],"metadata":{"id":"wXoJmIGl3CiY"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, default_data_collator\n","import numpy as np"],"metadata":{"id":"g-yClCn0z2FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name_or_path = './mlm_bert' # @param {type:\"string\"}\n","datatset_name = 'VietAI/vi_mednli'  # @param {type:\"string\"}\n","max_seq_length = 64   # @param {type:\"number\"}\n","\n","finetuning_model_data_args = ModelDataArguments(\n","                                    model_name_or_path=model_name_or_path,\n","                                    dataset_name=datatset_name,\n","                                    max_seq_length=max_seq_length,\n","                                    line_by_line=False,\n","                                    pad_to_max_length=True,\n","                                    overwrite_cache=False,\n","                                    cache_dir=None\n","                                    )"],"metadata":{"id":"w1frWm9zx20k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the finetuning phase, it is recommended to use lower learning rate compared to pre-training. The BERT authors suggest to use the learning rate of 2e-5, 3e-5, 5e-5, etc. You should tune that number to get the highest performance. In this experiment, I will choose 2e-5."],"metadata":{"id":"7PxP7RRP5emj"}},{"cell_type":"code","source":["# Path to save the finetuned models\n","output_model_dir = \"./nli_models\"  # @param {type:\"string\"}\n","\n","# Batch size GPU/TPU core/CPU training.\n","batch_size = 64  # @param {type:\"number\"}\n","\n","# The initial learning rate for Adam. Defaults to 5e-5.\n","learning_rate = 2e-5  # @param {type:\"number\"}\n","\n","# Total number of training epochs to perform (if not an integer, will perform the\n","# decimal part percents of the last epoch before stopping training). max_steps = 200_000,\n","num_train_epochs = 20  # @param {type:\"number\"}\n","\n","# How often to show logs. I will se this to plot history loss and calculate perplexity.\n","logging_steps = 100  # @param {type:\"number\"}\n","\n","# Number of updates steps before two checkpoint saves. Defaults to 500\n","save_steps = 100  # @param {type:\"number\"}\n","\n","# The weight decay to apply (if not zero).\n","weight_decay = 0.0  # @param {type:\"number\"}\n","\n","# Epsilon for the Adam optimizer. Defaults to 1e-8\n","adam_epsilon = 1e-8  # @param {type:\"number\"}\n","\n","# Maximum gradient norm (for gradient clipping). Defaults to 0.\n","max_grad_norm = 1.0  # @param {type:\"number\"}\n","\n","# The total number of saved models.\n","save_total_limit = 3  # @param {type:\"number\"}\n","\n","# Set prediction loss to `True` in order to return loss for perplexity calculation.\n","prediction_loss_only = False # @param {type: \"boolean\"}\n","\n","# Whether to run training or not.\n","do_train = True # @param {type: \"boolean\"}\n","\n","# Whether to run evaluation or not.\n","do_eval = True # @param {type: \"boolean\"}\n","\n","# Overwrite the content of the output directory.\n","overwrite_output_dir = True # @param {type: \"boolean\"}\n","\n","\n","# Define arguments for training\n","# `TrainingArguments` contains a lot more arguments.\n","# For more details check the awesome documentation:\n","# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n","downstream_training_args = TrainingArguments(\n","                          output_dir=output_model_dir,\n","                          overwrite_output_dir=overwrite_output_dir,\n","                          do_train=do_train,\n","                          do_eval=do_eval,\n","                          per_device_train_batch_size=batch_size,\n","                          logging_steps=logging_steps,\n","                          prediction_loss_only=prediction_loss_only,\n","                          learning_rate = learning_rate,\n","                          weight_decay=weight_decay,\n","                          adam_epsilon = adam_epsilon,\n","                          max_grad_norm = max_grad_norm,\n","                          num_train_epochs = num_train_epochs,\n","                          save_steps = save_steps,\n","                          save_total_limit = save_total_limit)\n","\n"],"metadata":{"id":"MauFIgPxywIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's load the finetuning dataset!\n","\n","We also need to re-process the dataset. At that time, we should keep both the text and the label."],"metadata":{"id":"GbA3fEaM5Bnt"}},{"cell_type":"code","source":["datasets = get_dataset(finetuning_model_data_args)\n","\n","label_list = datasets[\"train\"].unique(\"label\")\n","label_list.sort()  # Let's sort it for determinism\n","num_labels = len(label_list)\n","\n","\n","tokenized_datasets = preprocess_data(finetuning_model_data_args, downstream_training_args, datasets, tokenizer)\n","\n","# Split train/eval datasets\n","train_dataset, eval_dataset, test_dataset = tokenized_datasets['train'], tokenized_datasets['validation'], tokenized_datasets['test']\n","print('Training set:', len(train_dataset))\n","print('Validation set:', len(eval_dataset))\n","print('Test set:', len(test_dataset))"],"metadata":{"id":"RPBk67mxdvWZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset that we are working with is composed of a total of 14,049 samples, which have been partitioned into three subsets: 11,232 samples for training, 1,395 samples for validation, and 1,422 samples for testing. Each of these samples includes three columns, namely a `sentence1` column, `sentence2` column and a `label` column. The `label` column has three classes, including neutral, entailment, and contradiction."],"metadata":{"id":"r34OVtyKvv0Q"}},{"cell_type":"markdown","source":["Then let's initialize the Classifier. You should note that you will need to indicate the number of target labels. Don't forget that."],"metadata":{"id":"jmrjsZcE5M6R"}},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(\n","        model_name_or_path,\n","        num_labels=num_labels,\n","        finetuning_task='nli',\n","        cache_dir=finetuning_model_data_args.cache_dir,\n",")\n","\n","\n","classifier = AutoModelForSequenceClassification.from_pretrained(\n","        model_name_or_path,\n","        config=config,\n","        cache_dir=finetuning_model_data_args.cache_dir,\n","        ignore_mismatched_sizes=True,\n",")"],"metadata":{"id":"4SwwKL8Bw1uw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need a general-purpose function used to calculate various performance metrics for a given set of predictions and their corresponding ground-truth labels.\n","\n","This function is particularly necessary to evaluate the performance of a model on a test set of data. By computing relevant performance metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC, we can assess the quality of our model's predictions and make informed decisions about potential improvements.\n","\n","In this notebook, we will calcuate the `accuracy` metric to measure our trained models."],"metadata":{"id":"KUPJ63gy0p9t"}},{"cell_type":"code","source":["# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n","# predictions and label_ids field) and has to return a dictionary string to float.\n","def compute_metrics(p):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n"],"metadata":{"id":"L8_b2YxszV3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if we already did the padding.\n","\n","We now start finetuning the model on the Sentence-pair classification (**ViMedNLI**) dataset."],"metadata":{"id":"CIlgamdr0hv7"}},{"cell_type":"code","source":["# Initialize our Trainer\n","trainer = Trainer(model=classifier,\n","                  args=downstream_training_args,\n","                  data_collator=default_data_collator if finetuning_model_data_args.pad_to_max_length else None,\n","                  train_dataset=train_dataset,\n","                  eval_dataset=eval_dataset,\n","                  compute_metrics=compute_metrics,\n","                  )\n","\n","print('Start training...')\n","trainer.train()\n"],"metadata":{"id":"R8Qa9zq_xavw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Yeah! Our model has completed training. Now we can evaluate the model on the test dataset.\n","\n","Let's see how much we can achieve>>>"],"metadata":{"id":"FIqNfsEY4C7c"}},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"id":"Jy90OsM-39SJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The result looks good, right?!\n","\n","Congratulations! We have completed a very difficult challenge. Best of luck with your future career in NLP."],"metadata":{"id":"k0LEmgEs4abE"}},{"cell_type":"markdown","metadata":{"id":"GKRFG6qzCPvq"},"source":["## **References**\n","\n","This notebook is **very heavily inspired** from [gmihaila/ml_things](https://github.com/gmihaila/ml_things) and the [Hugging Face script](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) used for training language models. It is updated to work with the latest `transformers`.\n"]}]}